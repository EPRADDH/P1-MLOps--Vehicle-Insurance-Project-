
<<<<<<< HEAD

Key Features
ðŸ“¦ Project Templating: Auto-generated structure using template.py

ðŸ Python Virtual Environment: Easily setup with conda

ðŸ§ª Modular ML Pipeline: Clean separation of concerns â€” ingestion, transformation, training, evaluation

ðŸŒ MongoDB Atlas Integration: Centralized and scalable NoSQL storage

ðŸ”’ Custom Logging & Exception Handling

ðŸ“Š EDA & Feature Engineering: Notebook-ready analysis

ðŸ“¥ Data Ingestion via MongoDB âžœ Pandas

âœ… Data Validation with Schema YAML

ðŸ”„ Data Transformation + Model Training

â˜ï¸ AWS S3 for Model Registry

ðŸ§  Model Evaluation + Conditional Model Push

ðŸ§¾ Prediction API with FastAPI/Flask

ðŸ³ Dockerized App + .dockerignore

âš™ï¸ CI/CD using GitHub Actions + AWS ECR + EC2

ðŸ” IAM Role Management for secure access


Project Workflow
âœ… Phase 1: Project Setup
Set up modular Python structure with template.py.

Configure setup.py and pyproject.toml for package installation.

Create virtual environment: conda create -n vehicle python=3.10

Install all dependencies via requirements.txt.

âœ… Phase 2: MongoDB Integration
Use MongoDB Atlas to store raw data.
Step 1: MongoDB Atlas Configuration
Sign up for MongoDB Atlas and create a new project.
Set up a free M0 cluster, configure the username and password, and allow access from any IP address (0.0.0.0/0).
Retrieve the MongoDB connection string for Python and save it (replace <password> with your password).
Step 2: Pushing Data to MongoDB
Create a folder named notebook, add the dataset, and create a notebook file mongoDB_demo.ipynb.
Use the notebook to push data to the MongoDB database.
Verify the data in MongoDB Atlas under Database > Browse Collections.

âœ… Phase 3: Logging & Exception Handling
Implement centralized logging using logger.py.

Handle custom exceptions with exception.py.

âœ… Phase 4: Data Pipeline (ETL)
Data Ingestion

Connect to MongoDB, retrieve datasets, and save locally.

Data Validation

Validate schema using schema.yaml.

Data Transformation

Clean and transform features for model input.

âœ… Phase 5: Model Training & Evaluation
Use Scikit-learn models and transformers.

Evaluate models using thresholds defined in constants.

Save models as serialized .pkl objects using dill.

âœ… Phase 6: AWS Integration
Upload trained models to AWS S3.

Use IAM roles and secure keys for access.

Store and version model artifacts in S3 buckets.

âœ… Phase 7: Model Deployment
Build a FastAPI app to serve predictions.

Routes:

/ â†’ Homepage

/predict â†’ Vehicle insurance prediction

/train â†’ Trigger model training pipeline

âœ… Phase 8: CI/CD with GitHub Actions & AWS EC2
Setup GitHub Actions with AWS secrets.

Use Docker to containerize the application.

Use AWS EC2 (Ubuntu) as a self-hosted GitHub runner.

CI/CD flow:

Code pushed to GitHub.

GitHub Action triggers.

Docker image pushed to AWS ECR.

Image deployed to EC2 instance.

FastAPI app hosted on EC2 public IP (Port 5080).

ðŸŽ¯ Project Workflow Summary
Data Ingestion âž” Data Validation âž” Data Transformation
             âž” Model Training âž” Model Evaluation âž” Model Deployment
             âž” CI/CD Automation with GitHub Actions, Docker & AWS

---------------------------------------------------------------------
=======
>>>>>>> cfc24f98670431f053d6e525ebbe53ed271c5e0e
